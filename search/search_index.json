{"config":{"lang":["zh"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stemmer"]},"docs":[{"location":"index.html","title":"Langchain\u6837\u4f8b","text":"<ul> <li>\u5feb\u901f\u5f00\u59cb</li> <li>Langchain\u8868\u8fbe\u5f0f\u8bed\u8a00</li> </ul>"},{"location":"lcel.html","title":"LangChain Expression Language (LangChain\u8868\u8fbe\u5f0f\u8bed\u8a00)","text":"<p>LangChain \u8868\u8fbe\u5f0f\u8bed\u8a00\uff08LCEL\uff09\u662f\u4e00\u79cd\u8f7b\u677e\u5730\u5c06\u94fe\u7ec4\u5408\u5728\u4e00\u8d77\u7684\u58f0\u660e\u6027\u65b9\u5f0f\u3002 LCEL \u4ece\u7b2c\u4e00\u5929\u8d77\u5c31\u88ab\u8bbe\u8ba1\u4e3a**\u652f\u6301\u5c06\u539f\u578b\u6295\u5165\u751f\u4ea7\uff0c\u65e0\u9700\u66f4\u6539\u4ee3\u7801 **\uff0c\u4ece\u6700\u7b80\u5355\u7684\u201c\u63d0\u793a\u8bcd + LLM\u201d\u94fe\u5230\u6700\u590d\u6742\u7684\u94fe\uff08\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4eba\u4eec\u5728\u751f\u4ea7\u4e2d\u6210\u529f\u8fd0\u884c\u4e86 100 \u4e2a\u6b65\u9aa4\u7684 LCEL \u94fe\uff09\u3002\u4e0b\u9762\u662f\u4f7f\u7528 LCEL \u7684\u4e00\u4e9b\u539f\u56e0\uff1a</p> <ul> <li>\u652f\u6301\u6d41\u5f0f\u4f20\u8f93\uff08Streaming support\uff09\uff1a\u4f7f\u7528 LCEL \u6784\u5efa\u94fe\u65f6\uff0c\u60a8\u53ef\u4ee5\u83b7\u5f97\u6700\u77ed\u7684\u9996\u6b21\u4ee3\u5e01\u65f6\u95f4\uff08\u76f4\u5230\u7b2c\u4e00\u4e2a\u8f93\u51fa\u5757\u51fa\u73b0\u4e4b\u524d\u7ecf\u8fc7\u7684\u65f6\u95f4\uff09\u3002\u5bf9\u4e8e\u67d0\u4e9b   chain \u6765\u8bf4\uff0c\u76f4\u63a5\u5c06 token \u4ece LLM \u6d41\u5f0f\u4f20\u8f93\u5230\u6d41\u5f0f\u8f93\u51fa\u89e3\u6790\u5668\uff0c\u53ef\u4ee5\u4ee5 LLM \u63d0\u4f9b\u8005\u8f93\u51fa\u539f\u59cb toekn \u76f8\u540c\u7684\u901f\u7387\u8f93\u51fa\u89e3\u6790\u7684\u7ed3\u679c\u3002</li> <li>\u652f\u6301\u5f02\u6b65\uff08Async support\uff09\uff1aLCEL \u6784\u5efa\u7684\u4efb\u4f55\u94fe\u90fd\u53ef\u4ee5\u4f7f\u7528\u540c\u6b65 API\uff08\u4f8b\u5982\uff0c\u5728 Jupyter \u4e2d\u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u65f6\uff09\u548c\u5f02\u6b65 API\uff08\u4f8b\u5982\uff0c\u5728   LangServe \u670d\u52a1\u5668\u4e2d\uff09\u8fdb\u884c\u8c03\u7528\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u5728\u539f\u578b\u8bbe\u8ba1\u548c\u751f\u4ea7\u4e2d\u4f7f\u7528\u76f8\u540c\u7684\u4ee3\u7801\uff0c\u5e76\u4e14\u5177\u6709\u51fa\u8272\u7684\u6027\u80fd\uff0c\u80fd\u591f\u5728\u540c\u4e00\u670d\u52a1\u5668\u4e2d\u5904\u7406\u8bb8\u591a\u5e76\u53d1\u8bf7\u6c42\u3002</li> <li>\u4f18\u5316\u8fc7\u5e76\u884c\u6267\u884c\uff08Optimized parallel execution\uff09\uff1a\u53ea\u8981\u4f60\u7684 LCEL Chain   \u5177\u6709\u53ef\u4ee5\u5e76\u884c\u6267\u884c\u7684\u6b65\u9aa4\uff08\u4f8b\u5982\uff0c\u4ece\u591a\u4e2a\u68c0\u7d22\u5668\u83b7\u53d6\u6587\u6863\uff09\uff0c\u6211\u4eec\u5c31\u4f1a\u5728\u540c\u6b65\u548c\u5f02\u6b65\u63a5\u53e3\u4e2d\u81ea\u52a8\u6267\u884c\u6b64\u64cd\u4f5c\uff0c\u4ee5\u5c3d\u53ef\u80fd\u51cf\u5c11\u5ef6\u8fdf\u3002</li> <li>\u91cd\u8bd5\u548c\u56de\u9000\uff08Retries and fallbacks\uff09\uff1a\u4e3a LCEL Chain \u7684\u4efb\u4f55\u90e8\u5206\u914d\u7f6e\u91cd\u8bd5\u548c\u56de\u9000\u3002\u8fd9\u662f\u8ba9\u4f60\u7684 Chain \u5728\u89c4\u6a21\u5316\u7684\u65f6\u5019\u66f4\u52a0\u53ef\u9760\u7684\u597d\u65b9\u6cd5\u3002</li> <li>\u8f93\u5165\u548c\u8f93\u51fa\u6a21\u5f0f\uff08Input and output modes\uff09\uff1a\u8f93\u5165\u548c\u8f93\u51fa\u6a21\u5f0f\u4e3a\u6bcf\u4e2a LCEL Chain \u63d0\u4f9b\u4ece\u94fe\u7ed3\u6784\u63a8\u65ad\u51fa\u7684 Pydantic \u548c JSONSchema   \u6a21\u5f0f\u3002\u8fd9\u53ef\u7528\u4e8e\u9a8c\u8bc1\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u5e76\u4e14\u662f LangServe \u7684\u7ec4\u6210\u90e8\u5206\u3002</li> <li>\u65e0\u7f1d\u96c6\u6210 LangSmith\uff08Seamless LangSmith tracing integration\uff09\uff1a\u968f\u7740 Chain \u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u4e86\u89e3\u6bcf\u4e00\u6b65\u5230\u5e95\u53d1\u751f\u4e86\u4ec0\u4e48\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u501f\u52a9   LCEL\uff0c\u6240\u6709\u6b65\u9aa4\u90fd\u4f1a\u81ea\u52a8\u8bb0\u5f55\u5230 LangSmith\uff0c\u4ee5\u5b9e\u73b0\u6700\u5927\u7a0b\u5ea6\u7684\u53ef\u89c2\u5bdf\u6027\u548c\u53ef\u8c03\u8bd5\u6027\u3002</li> <li>\u65e0\u7f1d\u96c6\u6210LangServe\uff08Seamless LangServe deployment integration\uff09\uff1a\u4f7f\u7528 LCEL \u521b\u5efa\u7684\u4efb\u4f55 Chain \u90fd\u53ef\u4ee5\u4f7f\u7528 LangServe \u8f7b\u677e\u90e8\u7f72\u3002</li> </ul> <p>LCEL \u53ef\u4ee5\u8f7b\u677e\u5730\u4ece\u57fa\u672c\u7ec4\u4ef6\u6784\u5efa\u590d\u6742\u7684\u94fe\uff0c\u5e76\u652f\u6301\u5f00\u7bb1\u5373\u7528\u7684\u529f\u80fd\uff0c\u4f8b\u5982\u6d41\u5f0f\u4f20\u8f93\u3001\u5e76\u884c\u6027\u548c\u65e5\u5fd7\u8bb0\u5f55\u3002</p>"},{"location":"lcel.html#llm","title":"\u57fa\u7840\u6848\u4f8b: \u63d0\u793a\u8bcd+LLM+\u8f93\u51fa\u89e3\u6790\u5668","text":"<p>\u6700\u57fa\u672c\u548c\u5e38\u89c1\u7684\u7528\u4f8b\u662f\u5c06\u63d0\u793a\u6a21\u677f\u548c\u6a21\u578b\u94fe\u63a5\u5728\u4e00\u8d77\u3002\u4e3a\u4e86\u770b\u770b\u8fd9\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff0c\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u63a5\u53d7\u4e3b\u9898\u5e76\u751f\u6210\u7b11\u8bdd\u7684\u94fe\uff1a</p> <pre><code>import os\n\nfrom dotenv import load_dotenv\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\ndef test_basic_demo():\n    prompt = ChatPromptTemplate.from_template(\"\u7ed9\u6211\u5c06\u4e00\u4e2a\u5173\u4e8e{topic}\u7684\u7b11\u8bdd\")\n    model = ChatOpenAI()\n    output_parser = StrOutputParser()\n\n    chain = prompt | model | output_parser\n\n    res = chain.invoke({\"topic\": \"\u51b0\u6dc7\u6dcb\"})\n    print(f\"type(res): {type(res)}, res: {res}\")\n\n\ndef main():\n    load_dotenv()\n    test_basic_demo()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p></p> <p>\u8bf7\u6ce8\u610f\u8fd9\u884c\u4ee3\u7801\uff0c\u6211\u4eec\u4f7f\u7528 LCEL \u5c06\u4e0d\u540c\u7684\u7ec4\u4ef6\u62fc\u51d1\u6210\u4e00\u4e2a\u94fe\uff1a</p> <pre><code>chain = prompt | model | output_parser\n</code></pre> <p><code>|</code> \u7b26\u53f7\u7c7b\u4f3c\u4e8e unix \u7ba1\u9053\u8fd0\u7b97\u7b26\uff0c\u5b83\u5c06\u4e0d\u540c\u7684\u7ec4\u4ef6\u94fe\u63a5\u5728\u4e00\u8d77\uff0c\u5c06\u4e00\u4e2a\u7ec4\u4ef6\u7684\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u7684\u8f93\u5165\u3002</p> <p>\u5728\u6b64\u94fe\u4e2d\uff0c\u7528\u6237\u8f93\u5165\u4f20\u9012\u5230\u63d0\u793a\u8bcd\u6a21\u677f\uff0c\u7136\u540e\u63d0\u793a\u6a21\u677f\u8bcd\u8f93\u51fa\u4f20\u9012\u5230\u6a21\u578b\uff0c\u7136\u540e\u6a21\u578b\u8f93\u51fa\u4f20\u9012\u5230\u8f93\u51fa\u89e3\u6790\u5668\u3002\u8ba9\u6211\u4eec\u5206\u522b\u770b\u4e00\u4e0b\u6bcf\u4e2a\u7ec4\u4ef6\uff0c\u4ee5\u771f\u6b63\u4e86\u89e3\u53d1\u751f\u4e86\u4ec0\u4e48\u3002</p>"},{"location":"lcel.html#prompt","title":"\u63d0\u793a\u8bcd\uff08Prompt\uff09","text":"<p><code>prompt</code> \u662f\u4e00\u4e2a <code>BasePromptTemplate</code>\uff0c\u4ed6\u63a5\u53d7\u4e00\u4e2a\u5173\u4e8e\u6a21\u677f\u53d8\u91cf\u7684\u5b57\u5178\uff0c\u7136\u540e\u8fd4\u56de<code>PromptValue</code>\u3002</p> <p>PromptValue \u662f\u4e00\u4e2a\u5b8c\u6574\u63d0\u793a\u7684\u5305\u88c5\u5668\uff0c\u53ef\u4ee5\u4f20\u9012\u7ed9 LLM \uff08\u5b83\u63a5\u53d7\u4e00\u4e2a\u5b57\u7b26\u4e32\u4f5c\u4e3a\u8f93\u5165\uff09\u6216 ChatModel \uff08\u5b83\u63a5\u53d7\u4e00\u4e2a\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\u7684\u6d88\u606f\uff09\u3002</p> <p>\u5b83\u53ef\u4ee5\u4e0e\u4efb\u4f55\u4e00\u79cd\u8bed\u8a00\u6a21\u578b\u7c7b\u578b\u4e00\u8d77\u4f7f\u7528\uff0c\u56e0\u4e3a\u5b83\u5b9a\u4e49\u4e86\u751f\u6210 BaseMessage \u548c\u751f\u6210\u5b57\u7b26\u4e32\u7684\u903b\u8f91\u3002</p> <pre><code>from langchain_core.prompts import ChatPromptTemplate, BasePromptTemplate\n\n\ndef test_prompt():\n    prompt = ChatPromptTemplate.from_template(\"\u7ed9\u6211\u5c06\u4e00\u4e2a\u5173\u4e8e{topic}\u7684\u7b11\u8bdd\")\n    print(f\"{type(prompt)=}, {prompt=}\\n\")\n    print(f\"{isinstance(prompt, BasePromptTemplate)=}\\n\")\n    prompt_value = prompt.invoke({\"topic\": \"\u51b0\u6dc7\u6dcb\"})\n    print(f\"{type(prompt_value)=}, {prompt_value=}\\n\")\n    msgs = prompt_value.to_messages()\n    print(f\"{type(msgs)=}, {msgs=}\\n\")\n    s = prompt_value.to_string()\n    print(f\"{type(s)=}, {s=}\")\n\n\ndef main():\n    test_prompt()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p></p>"},{"location":"lcel.html#model","title":"\u6a21\u578b\uff08Model)","text":"<p>\u7136\u540e <code>PromptValue</code> \u88ab\u4f20\u9012\u7ed9 <code>model</code> \u3002</p> <p>\u5728\u672c\u4f8b\u4e2d\uff0c\u6211\u4eec\u7684 <code>model</code> \u662f <code>ChatModel</code> \uff0c\u8fd9\u610f\u5473\u7740\u5b83\u5c06\u8f93\u51fa <code>BaseMessage</code> \u3002</p> <p>\u5982\u679c\u6211\u4eec\u7684 <code>model</code> \u662f <code>LLM</code> \uff0c\u5b83\u5c06\u8f93\u51fa\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002</p> <pre><code>import os\n\nfrom dotenv import load_dotenv\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_community.llms.openai import OpenAI\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\ndef test_chat_model():\n    print(f\"{'*' * 20} test_chat_model {'*' * 20}\")\n    prompt = ChatPromptTemplate.from_template(\"\u7ed9\u6211\u5c06\u4e00\u4e2a\u5173\u4e8e{topic}\u7684\u7b11\u8bdd\")\n    prompt_value = prompt.invoke({\"topic\": \"\u51b0\u6dc7\u6dcb\"})\n    print(f\"{type(prompt_value)=}, {prompt_value=}\\n\")\n\n    model = ChatOpenAI()\n\n    message = model.invoke(prompt_value)\n    print(f\"{type(message)=}, message: {message}\")\n\n\ndef test_llm_model():\n    print(f\"{'*' * 20} test_llm_model {'*' * 20}\")\n    prompt = ChatPromptTemplate.from_template(\"\u7ed9\u6211\u5c06\u4e00\u4e2a\u5173\u4e8e{topic}\u7684\u7b11\u8bdd\")\n    prompt_value = prompt.invoke({\"topic\": \"\u51b0\u6dc7\u6dcb\"})\n    print(f\"{type(prompt_value)=}, {prompt_value=}\\n\")\n\n    model = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n\n    message = model.invoke(prompt_value)\n    print(f\"{type(message)=}, message: {message}\")\n\n\ndef main():\n    load_dotenv()\n    test_chat_model()\n    test_llm_model()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"lcel.html#output_parser","title":"\u8f93\u51fa\u89e3\u6790\u5668\uff08Output parser\uff09","text":"<p>\u6700\u540e\uff0c\u6211\u4eec\u5c06 <code>model</code> \u8f93\u51fa\u4f20\u9012\u7ed9 <code>output_parser</code> \uff0c\u8fd9\u662f\u4e00\u4e2a <code>BaseOutputParser</code> \uff0c\u610f\u5473\u7740\u5b83\u63a5\u53d7\u5b57\u7b26\u4e32\u6216 <code>BaseMessage</code> \u4f5c\u4e3a\u8f93\u5165\u3002</p> <p><code>StrOutputParser</code> \u7279\u522b\u7b80\u5355\u5730\u5c06\u4efb\u4f55\u8f93\u5165\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\u3002</p> <pre><code>import os\n\nfrom dotenv import load_dotenv\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_community.llms.openai import OpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\ndef test_chat_model():\n    print(f\"{'*' * 20} test_chat_model {'*' * 20}\")\n    prompt = ChatPromptTemplate.from_template(\"\u7ed9\u6211\u5c06\u4e00\u4e2a\u5173\u4e8e{topic}\u7684\u7b11\u8bdd\")\n    prompt_value = prompt.invoke({\"topic\": \"\u51b0\u6dc7\u6dcb\"})\n    print(f\"{type(prompt_value)=}, {prompt_value=}\\n\")\n\n    model = ChatOpenAI()\n\n    message = model.invoke(prompt_value)\n    print(f\"{type(message)=}, message: {message}\")\n    return message\n\n\ndef test_llm_model():\n    print(f\"{'*' * 20} test_llm_model {'*' * 20}\")\n    prompt = ChatPromptTemplate.from_template(\"\u7ed9\u6211\u5c06\u4e00\u4e2a\u5173\u4e8e{topic}\u7684\u7b11\u8bdd\")\n    prompt_value = prompt.invoke({\"topic\": \"\u51b0\u6dc7\u6dcb\"})\n    print(f\"{type(prompt_value)=}, {prompt_value=}\\n\")\n\n    model = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n\n    message = model.invoke(prompt_value)\n    print(f\"{type(message)=}, message: {message}\")\n    return message\n\n\ndef test_output_parser():\n    print(f\"{'*' * 20} test_output_parser {'*' * 20}\")\n    parser = StrOutputParser()\n\n    chat_message = test_chat_model()\n    chat_res = parser.invoke(chat_message)\n    print(f\"{type(chat_res)=}, {chat_res=}\")\n\n    llm_message = test_llm_model()\n    llm_res = parser.invoke(llm_message)\n    print(f\"{type(llm_res)=}, {llm_res=}\")\n\n\ndef main():\n    load_dotenv()\n    test_output_parser()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p></p>"},{"location":"lcel.html#entirte_pipeline","title":"\u5b8c\u6574\u7684\u7ba1\u9053\uff08Entirte Pipeline)","text":"<p>\u5b89\u88c5\u4ee5\u4e0b\u7684\u6b65\u9aa4\u8fdb\u884c\uff1a</p> <ol> <li>\u5c06\u7528\u6237\u8f93\u5165\u7684 topic \u6784\u9020\u6210\u5b57\u5178 <code>{\"topic\": \"\u51b0\u6dc7\u6dcb\"}</code> \u4f20\u5165 <code>PromptTemplate</code>\u3002</li> <li><code>prompt</code> \u7ec4\u4ef6\u63a5\u6536\u7528\u6237\u8f93\u5165\u7136\u540e\u6784\u9020\u6210\u4e00\u4e2a <code>PromptValue</code> \u5bf9\u8c61\u3002</li> <li><code>model</code> \u7ec4\u4ef6\u63a5\u6536\u751f\u6210\u7684\u63d0\u793a\u8bcd\uff0c\u7136\u540e\u5c06\u5176\u4f20\u9012\u7ed9 LLM \uff0c\u5e76\u5c06\u5176\u8f93\u51fa\u8f6c\u6362\u6210 <code>BaseMessage</code> \u5bf9\u8c61\u3002</li> <li><code>output_parser</code> \u7ec4\u4ef6\u63a5\u6536 <code>BaseMessage</code> \u5bf9\u8c61\u6216\u5b57\u7b26\u4e32\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u6210 Python \u5b57\u7b26\u4e32\u3002</li> </ol> <pre><code>graph LR\n    A(Input: topic=\u51b0\u6dc7\u6dcb) --&gt; |Dict| B(PromptTemplate)\n    B --&gt;|PromptValue| C(ChatModel)    \n    C --&gt;|BaseMessage| D(StrOutputParser)\n    D --&gt; |String| F(Result)</code></pre> <p>\u8bf7\u6ce8\u610f\uff0c\u5982\u679c\u60a8\u5bf9\u4efb\u4f55\u7ec4\u4ef6\u7684\u8f93\u51fa\u611f\u5230\u597d\u5947\uff0c\u60a8\u59cb\u7ec8\u53ef\u4ee5\u6d4b\u8bd5\u94fe\u7684\u8f83\u5c0f\u7248\u672c\uff0c\u4f8b\u5982 <code>prompt</code> \u6216 <code>prompt | model</code> \u4ee5\u67e5\u770b\u4e2d\u95f4\u7ed3\u679c\uff1a</p> <pre><code>input = {\"topic\": \"ice cream\"}\n\nprompt.invoke(input)\n\n(prompt | model).invoke(input)\n</code></pre>"},{"location":"lcel.html#rag","title":"RAG\u641c\u7d22\u793a\u4f8b","text":"<p>\u5bf9\u4e8e\u6211\u4eec\u7684\u4e0b\u4e00\u4e2a\u793a\u4f8b\uff0c\u6211\u4eec\u5e0c\u671b\u8fd0\u884c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u94fe\u4ee5\u5728\u56de\u7b54\u95ee\u9898\u65f6\u6dfb\u52a0\u4e00\u4e9b\u4e0a\u4e0b\u6587\u3002</p> <p>\u9996\u5148\u9700\u8981\u5b89\u88c5\u4f9d\u8d56\uff1a <pre><code>pip install langchain docarray tiktoken\n# langchain 0.0.352 \u8fd0\u884c\u4e0b\u9762\u7684\u6848\u5217\u4f1a\u6709bug\uff0c\u9700\u8981\u5c06pydantic\u7248\u672c\u964d\u52301.10.8\npip install pydantic==1.10.8\n# \u7531\u4e8eOpenai API\u63a5\u53e3\u6bcf\u5206\u949f\u9650\u52363\u6b21\uff0c\u5c06\u6a21\u578b\u6362\u6210\u963f\u91cc\u4e91\u901a\u7528\u5343\u95ee\uff0c\u9700\u8981\u5b89\u88c5dashscope\npip install dashscope==1.13.6\n</code></pre></p> <pre><code>import os\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain_community.chat_models.tongyi import ChatTongyi\nfrom langchain_community.embeddings import DashScopeEmbeddings\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableSequence\nfrom langchain_core.vectorstores import VectorStoreRetriever\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\ndef get_test_texts():\n    \"\"\"\u83b7\u53d6\u6d4b\u8bd5\u7528\u7684\u6587\u672c\u4fe1\u606f\"\"\"\n    texts = [\n        \"\u7334\u5b50\u5403\u9999\u8549\u201d\",\n        \"\u732b\u5403\u8001\u9f20\",\n        \"\u72d7\u5403\u9aa8\u5934\",\n        \"\u738b\u516b\u5403\u7eff\u8c46\u7cd5\"\n    ]\n    return texts\n\n\ndef get_retriever(texts: t.List[str]) -&gt; VectorStoreRetriever:\n    \"\"\"\u83b7\u53d6\u68c0\u7d22\u5668\n\n    Args:\n        texts: \u5411\u91cf\u5e93\u4e2d\u5b58\u50a8\u7684\u6587\u672c\u5185\u5bb9\n\n    Returns: VectorStoreRetriever\n\n    \"\"\"\n    vectorstore = DocArrayInMemorySearch.from_texts(texts, embedding=DashScopeEmbeddings())\n    retriever = vectorstore.as_retriever()\n    return retriever\n\n\ndef get_prompt_template() -&gt; ChatPromptTemplate:\n    template = \"\"\"\n    &lt;\u6307\u4ee4&gt;\n        \u6839\u636e\u5df2\u77e5\u4fe1\u606f\uff0c\u7b80\u6d01\u548c\u4e13\u4e1a\u7684\u6765\u56de\u7b54\u95ee\u9898\u3002\n        \u5982\u679c\u65e0\u6cd5\u4ece\u4e2d\u5f97\u5230\u7b54\u6848\uff0c\u8bf7\u8bf4 \u201c\u6839\u636e\u5df2\u77e5\u4fe1\u606f\u65e0\u6cd5\u56de\u7b54\u8be5\u95ee\u9898\u201d\uff0c\u4e0d\u5141\u8bb8\u5728\u7b54\u6848\u4e2d\u6dfb\u52a0\u7f16\u9020\u6210\u5206\uff0c\u7b54\u6848\u8bf7\u4f7f\u7528\u4e2d\u6587\u3002\n    &lt;/\u6307\u4ee4&gt;\n    &lt;\u5df2\u77e5\u4fe1\u606f&gt;\n        {context}\n    &lt;/\u5df2\u77e5\u4fe1\u606f&gt;\n    &lt;\u95ee\u9898&gt;\n        {question}\n    &lt;/\u95ee\u9898&gt;\n    \"\"\"\n    prompt = ChatPromptTemplate.from_template(template)\n    return prompt\n\n\ndef get_model():\n    model = ChatTongyi()\n    return model\n\n\ndef get_output_parser():\n    output_parser = StrOutputParser()\n    return output_parser\n\n\ndef get_rag_chain(retriever: VectorStoreRetriever) -&gt; RunnableSequence:\n    \"\"\"\u83b7\u53d6rag chain\n\n    Args:\n        retriever: \u68c0\u7d22\u5668\n\n    Returns: RunnableSequence\n\n    \"\"\"\n    prompt = get_prompt_template()\n    model = get_model()\n    output_parser = get_output_parser()\n    setup_and_retrieval = RunnableParallel(\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\n    )\n    chain = setup_and_retrieval | prompt | model | output_parser\n    print(f\"{type(chain)=}, {chain=}\")\n    return chain\n\n\ndef test_chain(chain, user_inputs: t.List[str]):\n    outputs = chain.batch(user_inputs)\n    for i, output in enumerate(outputs):\n        user_input = user_inputs[i]\n        print(f\"Q:{user_input}\\nA:{output}\")\n\n\ndef main():\n    load_env()\n    texts = get_test_texts()\n    retriever = get_retriever(texts)\n    chain = get_rag_chain(retriever)\n    test_inputs = [\"\u732b\u5403\u4ec0\u4e48\uff1f\",\n                   \"\u738b\u516b\u5403\u4ec0\u4e48\uff1f\",\n                   \"\u732a\u5403\u4ec0\u4e48\uff1f\"]\n    test_chain(chain, test_inputs)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p></p>"},{"location":"lcel.html#lcel","title":"\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 LCEL","text":"<p>LCEL \u53ef\u4ee5\u8f7b\u677e\u5730\u4ece\u57fa\u672c\u7ec4\u4ef6\u6784\u5efa\u590d\u6742\u7684\u94fe\u6761\u3002\u5b83\u901a\u8fc7\u63d0\u4f9b\u4ee5\u4e0b\u529f\u80fd\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff1a 1. \u7edf\u4e00\u7684\u63a5\u53e3\uff1a\u6bcf\u4e2a LCEL \u5bf9\u8c61\u90fd\u5b9e\u73b0 Runnable \u63a5\u53e3\uff0c\u8be5\u63a5\u53e3\u5b9a\u4e49\u4e86\u4e00\u7ec4\u901a\u7528\u7684\u8c03\u7528\u65b9\u6cd5\uff08 invoke \u3001 batch \u3001 stream \u3001 ainvoke \u3001...\uff09\u3002\u8fd9\u4f7f\u5f97 LCEL \u5bf9\u8c61\u94fe\u4e5f\u53ef\u4ee5\u81ea\u52a8\u652f\u6301\u8fd9\u4e9b\u8c03\u7528\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u6bcf\u4e2a LCEL \u5bf9\u8c61\u94fe\u672c\u8eab\u5c31\u662f\u4e00\u4e2a LCEL \u5bf9\u8c61\u3002 2. \u7ec4\u5408\u539f\u8bed\uff1aLCEL \u63d0\u4f9b\u4e86\u8bb8\u591a\u539f\u8bed\uff0c\u53ef\u4ee5\u8f7b\u677e\u7ec4\u5408\u94fe\u3001\u5e76\u884c\u5316\u7ec4\u4ef6\u3001\u6dfb\u52a0\u540e\u7f6e\u64cd\u4f5c\u3001\u52a8\u6001\u914d\u7f6e\u94fe\u5185\u90e8\u7b49\u3002</p> <p>\u5177\u4f53\u770b\u53c2\u8003\u94fe\u63a5\uff1ahttps://python.langchain.com/docs/expression_language/why\uff0c\u8fd9\u4e2a\u91cc\u9762\u4ecb\u7ecd\u4e86\u4f7f\u7528 LCEL \u5e26\u6765\u7684\u4e00\u4e9b\u597d\u5904\u3002</p>"},{"location":"quick_start.html","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>\u5728\u672c\u5feb\u901f\u5165\u95e8\u4e2d\uff0c\u6211\u4eec\u5c06\u5411\u60a8\u5c55\u793a\u5982\u4f55\uff1a</p> <ul> <li>\u4f7f\u7528 LangChain\u3001LangSmith \u548c LangServe \u8fdb\u884c\u8bbe\u7f6e</li> <li>\u4f7f\u7528LangChain\u6700\u57fa\u672c\u3001\u6700\u5e38\u7528\u7684\u7ec4\u4ef6\uff1a\u63d0\u793a\u6a21\u677f\uff08prompt templates)\u3001\u6a21\u578b\uff08models\uff09\u548c\u8f93\u51fa\u89e3\u6790\u5668\uff08output parsers)</li> <li>\u4f7f\u7528 LangChain \u8868\u8fbe\u5f0f\u8bed\u8a00\uff0c\u8fd9\u662f LangChain \u6784\u5efa\u7684\u534f\u8bae\uff0c\u6709\u52a9\u4e8e\u7ec4\u4ef6\u94fe\u63a5</li> <li>\u4f7f\u7528LangChain\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u5e94\u7528\u7a0b\u5e8f</li> <li>\u4f7f\u7528 LangSmith \u8ffd\u8e2a\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f</li> <li>\u4f7f\u7528 LangServe \u4e3a\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u670d\u52a1</li> </ul>"},{"location":"quick_start.html#_2","title":"\u8bbe\u7f6e","text":""},{"location":"quick_start.html#_3","title":"\u5b89\u88c5","text":"<pre><code>pip install langchain\n</code></pre> <p>\u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u6211\u4eec\u7684\u5b89\u88c5\u6307\u5357\u3002</p>"},{"location":"quick_start.html#_4","title":"\u73af\u5883\u53d8\u91cf","text":"<p>\u4f7f\u7528 LangChain \u901a\u5e38\u9700\u8981\u4e0e\u4e00\u4e2a\u6216\u591a\u4e2a\u6a21\u578b\u63d0\u4f9b\u8005\uff08model providers\uff09\u3001\u6570\u636e\u5b58\u50a8\uff08data stores\uff09\u3001API\uff08APIs\uff09 \u7b49\u96c6\u6210\u3002\u5728\u672c\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 OpenAI \u7684\u6a21\u578b API\u3002</p> <p>\u9996\u5148\u6211\u4eec\u9700\u8981\u5b89\u88c5\u4ed6\u4eec\u7684 Python \u5305\uff1a</p> <pre><code>pip install openai\n</code></pre> <p>\u8bbf\u95ee API \u9700\u8981 API \u5bc6\u94a5\u3002</p> <p>\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a <code>.env</code> \u7684\u6587\u4ef6\uff0c\u5e76\u5c06\u4ee5\u4e0b\u5185\u5bb9\u6dfb\u52a0\u5230\u5176\u4e2d\uff1a</p> <pre><code>OPENAI_API_KEY=YOUR_API_KEY\n</code></pre>"},{"location":"quick_start.html#langsmith","title":"LangSmith","text":"<p>\u60a8\u4f7f\u7528 LangChain \u6784\u5efa\u7684\u8bb8\u591a\u5e94\u7528\u7a0b\u5e8f\u5c06\u5305\u542b\u591a\u4e2a\u6b65\u9aa4\u4ee5\u53ca\u591a\u6b21\u8c03\u7528 LLM \u8c03\u7528\u3002\u968f\u7740\u8fd9\u4e9b\u5e94\u7528\u7a0b\u5e8f\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u80fd\u591f\u68c0\u67e5\u94fe\u6216\u4ee3\u7406\u5185\u90e8\u5230\u5e95\u53d1\u751f\u4e86\u4ec0\u4e48\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u505a\u5230\u8fd9\u4e00\u70b9\u7684\u6700\u4f73\u65b9\u6cd5\u662f\u4e0e LangSmith \u5408\u4f5c\u3002</p> <p>\u8bf7\u6ce8\u610f\uff0cLangSmith \u4e0d\u662f\u5fc5\u9700\u7684\uff0c\u4f46\u5b83\u5f88\u6709\u5e2e\u52a9\u3002\u5982\u679c\u60a8\u786e\u5b9e\u60f3\u4f7f\u7528 LangSmith\uff0c\u8bf7\u5728\u4e0a\u9762\u7684\u94fe\u63a5\u6ce8\u518c\u540e\uff0c\u786e\u4fdd\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u4ee5\u5f00\u59cb\u8bb0\u5f55\u8ddf\u8e2a\uff1a</p> <pre><code>export LANGCHAIN_TRACING_V2=\"true\"\nexport LANGCHAIN_API_KEY=\"...\"\n</code></pre>"},{"location":"quick_start.html#langserve","title":"LangServe","text":"<p>LangServe\u5e2e\u52a9\u5f00\u53d1\u8005\u5c06LangChain\u94fe\u90e8\u7f72\u4e3aREST API\u3002\u60a8\u4e0d\u9700\u8981\u4f7f\u7528 LangServe \u6765\u4f7f\u7528 LangChain\uff0c\u4f46\u5728\u672c\u6307\u5357\u4e2d\uff0c\u6211\u4eec\u5c06\u5411\u60a8\u5c55\u793a\u5982\u4f55\u4f7f\u7528 LangServe \u90e8\u7f72\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u3002</p> <p>\u5b89\u88c5\u547d\u4ee4:</p> <pre><code>pip install \"langserve[all]\"\n</code></pre>"},{"location":"quick_start.html#langchain","title":"\u4f7f\u7528 LangChain \u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u5e94\u7528\u7a0b\u5e8f","text":"<p>LangChain\u63d0\u4f9b\u4e86\u8bb8\u591a\u53ef\u7528\u4e8e\u6784\u5efa\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u7a0b\u5e8f\u7684\u6a21\u5757\u3002\u6a21\u5757\u53ef\u4ee5\u5728\u7b80\u5355\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u72ec\u7acb\u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u7ec4\u5408\u8d77\u6765\u7528\u4e8e\u66f4\u590d\u6742\u7684\u7528\u4f8b\u3002 \u5e94\u7528\u7531LangChain\u8868\u8fbe\u5f0f\u8bed\u8a00\uff08LangChain Expression Language: LCEL\uff09\u63d0\u4f9b\u652f\u6301\uff0c\u5b83\u5b9a\u4e49\u4e86\u8bb8\u591a\u6a21\u5757\u5b9e\u73b0\u7684\u7edf\u4e00 <code>Runnable</code> \u63a5\u53e3\uff0c\u4f7f\u5f97\u65e0\u7f1d\u94fe\u63a5\u7ec4\u4ef6\u6210\u4e3a\u53ef\u80fd\u3002</p> <p>\u6700\u7b80\u5355\u3001\u6700\u5e38\u89c1\u7684\u94fe\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1a</p> <ul> <li>LLM/Chat Model\uff1a\u8bed\u8a00\u6a21\u578b\u662f\u8fd9\u91cc\u7684\u6838\u5fc3\u63a8\u7406\u5f15\u64ce\u3002\u4e3a\u4e86\u4f7f\u7528 LangChain\uff0c\u60a8\u9700\u8981\u4e86\u89e3\u4e0d\u540c\u7c7b\u578b\u7684\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528\u5b83\u4eec</li> <li>Prompt Template\uff1a\u8fd9\u5411\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6307\u4ee4\u3002\u8fd9\u63a7\u5236\u7740\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u5185\u5bb9\uff0c\u56e0\u6b64\u4e86\u89e3\u5982\u4f55\u6784\u5efa\u63d0\u793a\u548c\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002</li> <li>Output Parser\uff1a\u5c06\u6765\u81ea\u8bed\u8a00\u6a21\u578b\u7684\u539f\u59cb\u54cd\u5e94\u8f6c\u6362\u4e3a\u66f4\u53ef\u884c\u7684\u683c\u5f0f\uff0c\u4ece\u800c\u53ef\u4ee5\u8f7b\u677e\u4f7f\u7528\u4e0b\u6e38\u7684\u8f93\u51fa\u3002</li> </ul> <p>\u5728\u672c\u6307\u5357\u4e2d\uff0c\u6211\u4eec\u5c06\u5206\u522b\u4ecb\u7ecd\u8fd9\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u7136\u540e\u4ecb\u7ecd\u5982\u4f55\u7ec4\u5408\u5b83\u4eec\u3002\u4e86\u89e3\u8fd9\u4e9b\u6982\u5ff5\u5c06\u4e3a\u60a8\u4f7f\u7528\u548c\u5b9a\u5236 LangChain \u5e94\u7528\u7a0b\u5e8f\u505a\u597d\u51c6\u5907\u3002\u5927\u591a\u6570 LangChain \u5e94\u7528\u7a0b\u5e8f\u90fd\u5141\u8bb8\u60a8\u914d\u7f6e\u6a21\u578b\u548c/\u6216\u63d0\u793a\uff0c\u56e0\u6b64\u4e86\u89e3\u5982\u4f55\u5229\u7528\u8fd9\u4e00\u70b9\u5c06\u662f\u4e00\u4e2a\u5f88\u5927\u7684\u63a8\u52a8\u56e0\u7d20\u3002</p>"},{"location":"quick_start.html#llmchat_model","title":"LLM/Chat Model","text":"<p>\u6709\u4e24\u79cd\u7c7b\u578b\u7684\u8bed\u8a00\u6a21\u578b\uff1a</p> <ul> <li><code>LLM</code>: \u5e95\u5c42\u6a21\u578b\u5c06\u5b57\u7b26\u4e32\u4f5c\u4e3a\u8f93\u5165\u5e76\u8fd4\u56de\u5b57\u7b26\u4e32</li> <li><code>ChatModel</code>: \u5e95\u5c42\u6a21\u578b\u5c06\u6d88\u606f\u5217\u8868\u4f5c\u4e3a\u8f93\u5165\u5e76\u8fd4\u56de\u6d88\u606f</li> </ul> <p>\u5b57\u7b26\u4e32\u5f88\u7b80\u5355\uff0c\u4f46\u662f\u6d88\u606f\u5230\u5e95\u662f\u4ec0\u4e48\uff1f\u57fa\u672c\u6d88\u606f\u63a5\u53e3\u7531 <code>BaseMessage</code> \u5b9a\u4e49\uff0c\u5b83\u6709\u4e24\u4e2a\u5fc5\u9700\u7684\u5c5e\u6027\uff1a</p> <ul> <li><code>content</code>\uff1a\u6d88\u606f\u7684\u5185\u5bb9\u3002\u901a\u5e38\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002</li> <li><code>role</code>: <code>BaseMessage</code> \u6240\u6765\u81ea\u7684\u5b9e\u4f53\u3002</li> </ul> <p>LangChain\u63d0\u4f9b\u4e86\u51e0\u4e2a\u5bf9\u8c61\u6765\u65b9\u4fbf\u533a\u5206\u4e0d\u540c\u7684\u89d2\u8272\uff1a</p> <ul> <li><code>HumanMessage</code>\uff1a\u6765\u81ea\u4eba\u7c7b/\u7528\u6237\u7684 <code>BaseMessage</code> \u3002</li> <li><code>AIMessage</code>\uff1a\u6765\u81eaAI/\u52a9\u624b\u7684 <code>BaseMessage</code> \u3002</li> <li><code>SystemMessage</code>\uff1a\u6765\u81ea\u7cfb\u7edf\u7684 <code>BaseMessage</code> \u3002</li> <li><code>FunctionMessage</code> / <code>ToolMessage</code>\uff1a\u5305\u542b\u51fd\u6570\u6216\u5de5\u5177\u8c03\u7528\u7684\u8f93\u51fa\u7684 <code>BaseMessage</code>\u3002</li> </ul> <p>\u5982\u679c\u8fd9\u4e9b\u89d2\u8272\u542c\u8d77\u6765\u90fd\u4e0d\u6b63\u786e\uff0c\u8fd8\u6709\u4e00\u4e2a <code>ChatMessage</code> \u7c7b\uff0c\u60a8\u53ef\u4ee5\u5728\u5176\u4e2d\u624b\u52a8\u6307\u5b9a\u89d2\u8272\u3002</p> <p>LangChain \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7531 <code>LLM</code> \u548c <code>ChatModel</code> \u5171\u4eab\u7684\u901a\u7528\u63a5\u53e3\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u6700\u6709\u6548\u5730\u4e3a\u7ed9\u5b9a\u7684\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u63d0\u793a\uff0c\u7406\u89e3\u5dee\u5f02\u662f\u5f88\u6709\u7528\u7684\u3002</p> <p>\u8c03\u7528 <code>LLM</code> \u6216 <code>ChatModel</code> \u7684\u6700\u7b80\u5355\u65b9\u6cd5\u662f\u4f7f\u7528 <code>.invoke()</code>\uff0c\u8fd9\u662f\u6240\u6709 LangChain \u8868\u8fbe\u5f0f\u8bed\u8a00\uff08LCEL\uff09\u5bf9\u8c61\u7684\u901a\u7528\u540c\u6b65\u8c03\u7528\u65b9\u6cd5:</p> <ul> <li><code>LLM.invoke</code>\uff1a\u63a5\u6536\u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c\u8fd4\u56de\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002</li> <li><code>ChatModel.invoke</code>\uff1a\u63a5\u53d7 <code>BaseMessage</code> \u5217\u8868\uff0c\u8fd4\u56de <code>BaseMessage</code> \u3002</li> </ul> <p><code>LLM.invoke</code> \u548c <code>ChatModel.invoke</code> \u5b9e\u9645\u4e0a\u90fd\u652f\u6301 <code>Union[str, List[BaseMessage], PromptValue]</code> \u4f5c\u4e3a\u8f93\u5165\u3002 <code>PromptValue</code> \u662f\u4e00\u4e2a\u5bf9\u8c61\uff0c\u5b83\u5b9a\u4e49\u4e86\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u903b\u8f91\uff0c\u7528\u4e8e\u5c06\u5176\u8f93\u5165\u4f5c\u4e3a\u5b57\u7b26\u4e32\u6216\u6d88\u606f\u8fd4\u56de\u3002 <code>LLM</code> \u5177\u6709\u5c06\u5176\u4e2d\u4efb\u4f55\u4e00\u4e2a\u5f3a\u5236\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\u7684\u903b\u8f91\uff0c\u800c <code>ChatModel</code> \u5177\u6709\u5c06\u5176\u4e2d\u4efb\u4f55\u4e00\u4e2a\u5f3a\u5236\u8f6c\u6362\u4e3a\u6d88\u606f\u7684\u903b\u8f91\u3002 <code>LLM</code> \u548c <code>ChatModel</code> \u63a5\u53d7\u76f8\u540c\u7684\u8f93\u5165\u8fd9\u4e00\u4e8b\u5b9e\u610f\u5473\u7740\u60a8\u53ef\u4ee5\u5728\u5927\u591a\u6570\u94fe\u4e2d\u76f4\u63a5\u5c06\u5b83\u4eec\u5f7c\u6b64\u4ea4\u6362\u800c\u4e0d\u4f1a\u7834\u574f\u4efb\u4f55\u5185\u5bb9\uff0c\u5c3d\u7ba1\u8003\u8651\u8f93\u5165\u7684\u65b9\u5f0f\u5f53\u7136\u5f88\u91cd\u8981\u53d7\u5230\u5f3a\u5236\u4ee5\u53ca\u8fd9\u53ef\u80fd\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002 \u8981\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u6a21\u578b\uff0c\u8bf7\u524d\u5f80\u8bed\u8a00\u6a21\u578b\u90e8\u5206\u3002</p> <p>\u8fd9\u4e9b\u65b9\u6cd5\u7684\u8f93\u5165\u7c7b\u578b\u5b9e\u9645\u4e0a\u6bd4\u8fd9\u66f4\u901a\u7528\uff0c\u4f46\u4e3a\u4e86\u7b80\u5355\u8d77\u89c1\uff0c\u6211\u4eec\u53ef\u4ee5\u5047\u8bbe <code>LLM</code> \u4ec5\u63a5\u53d7\u5b57\u7b26\u4e32\uff0c\u800c <code>Chat</code> \u6a21\u578b\u4ec5\u63a5\u53d7\u6d88\u606f\u5217\u8868\u3002</p> <p>\u8ba9\u6211\u4eec\u770b\u770b\u5982\u4f55\u4f7f\u7528\u8fd9\u4e9b\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u578b\u548c\u8fd9\u4e9b\u4e0d\u540c\u7c7b\u578b\u7684\u8f93\u5165\u3002\u9996\u5148\uff0c\u6211\u4eec\u5bfc\u5165\u4e00\u4e2a <code>LLM</code> \u548c\u4e00\u4e2a <code>ChatModel</code>\u3002</p> <p><code>LLM</code> \u548c <code>ChatModel</code> \u5bf9\u8c61\u662f\u6709\u6548\u7684\u914d\u7f6e\u5bf9\u8c61\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>temperature</code> \u7b49\u53c2\u6570\u521d\u59cb\u5316\u5b83\u4eec\uff0c\u7136\u540e\u4f20\u9012\u5b83\u4eec\u3002</p> <pre><code>import os\n\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom dotenv import load_dotenv\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\ndef test_llm_and_chat_model():\n    llm = OpenAI()\n    chat_model = ChatOpenAI()\n\n    text = \"\u5bf9\u4e8e\u4e00\u5bb6\u751f\u4ea7\u5f69\u8272\u889c\u5b50\u7684\u516c\u53f8\u6765\u8bf4\uff0c\u4e00\u4e2a\u597d\u7684\u516c\u53f8\u540d\u79f0\u662f\u4ec0\u4e48\uff1f\"\n    messages = [HumanMessage(content=text)]\n\n    llm_res = llm.invoke(text)\n    print(llm_res)\n\n    chat_res = chat_model.invoke(messages)\n    print(chat_res)\n\n\ndef main():\n    # \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\n    load_env()\n\n    test_llm_and_chat_model()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p></p>"},{"location":"quick_start.html#prompt_template","title":"Prompt Template(\u63d0\u793a\u8bcd\u6a21\u677f)","text":"<p>\u5927\u591a\u6570 <code>LLM</code> \u5e94\u7528\u4e0d\u4f1a\u76f4\u63a5\u5c06\u7528\u6237\u8f93\u5165\u4f20\u9012\u7ed9 <code>LLM</code> \u3002\u901a\u5e38\uff0c\u4ed6\u4eec\u4f1a\u5c06\u7528\u6237\u8f93\u5165\u6dfb\u52a0\u5230\u8f83\u5927\u7684\u6587\u672c\u4e2d\uff0c\u79f0\u4e3a\u63d0\u793a\u6a21\u677f\uff08prompt template\uff09\uff0c\u8be5\u6587\u672c\u63d0\u4f9b\u6709\u5173\u5f53\u524d\u7279\u5b9a\u4efb\u52a1\u7684\u9644\u52a0\u4e0a\u4e0b\u6587\u3002</p> <p>\u5728\u524d\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f20\u9012\u7ed9\u6a21\u578b\u7684\u6587\u672c\u5305\u542b\u751f\u6210\u516c\u53f8\u540d\u79f0\u7684\u6307\u4ee4\u3002\u5982\u679c\u5e94\u7528\u7a0b\u5e8f\u662f\u4e00\u4e2a\u7ed9\u516c\u53f8\u8d77\u540d\u7684\u9700\u6c42\uff0c\u6839\u636e\u7528\u6237\u8f93\u5165\u7684\u4ea7\u54c1\u540d\u79f0\u6765\u7ed9\u516c\u53f8\u53d6\u540d\uff0c\u53ef\u4ee5\u7528\u63d0\u793a\u8bcd\u6a21\u677f\uff08Prompt Templates\uff09\u89e3\u51b3\u3002</p> <pre><code>import os\n\nfrom langchain.prompts import PromptTemplate\n\nfrom dotenv import load_dotenv\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\ndef test_prompt_template():\n    prompt = PromptTemplate.from_template(\"\u5bf9\u4e8e\u4e00\u5bb6\u751f\u4ea7{product}\u7684\u516c\u53f8\u6765\u8bf4\uff0c\u4e00\u4e2a\u597d\u7684\u516c\u53f8\u540d\u79f0\u662f\u4ec0\u4e48\uff1f\")\n    s = prompt.format(product=\"\u5f69\u8272\u889c\u5b50\")\n    print(s)\n\n\ndef main():\n    # \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\n    load_env()\n\n    test_prompt_template()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p></p> <p>\u4e0e Python \u539f\u59cb\u5b57\u7b26\u4e32\u683c\u5f0f\u5316\u76f8\u6bd4\uff0c\u4f7f\u7528\u5b83\u4eec\u6709\u51e0\u4e2a\u4f18\u70b9\u3002\u60a8\u53ef\u4ee5\u201c\u90e8\u5206\u201d\u8f93\u51fa\u53d8\u91cf - \u4f8b\u5982\u60a8\u4e00\u6b21\u53ea\u80fd\u683c\u5f0f\u5316\u90e8\u5206\u53d8\u91cf\u3002\u60a8\u53ef\u4ee5\u5c06\u5b83\u4eec\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u8f7b\u677e\u5730\u5c06\u4e0d\u540c\u7684\u6a21\u677f\u7ec4\u5408\u6210\u4e00\u4e2a\u63d0\u793a\u3002\u6709\u5173\u8fd9\u4e9b\u529f\u80fd\u7684\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u6709\u5173\u63d0\u793a\u8bcd \u7684\u90e8\u5206\u4ee5\u4e86\u89e3\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002</p> <p><code>PromptTemplate</code> \u4e5f\u53ef\u7528\u4e8e\u751f\u6210\u6d88\u606f\u5217\u8868\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u63d0\u793a\u4e0d\u4ec5\u5305\u542b\u6709\u5173\u5185\u5bb9\u7684\u4fe1\u606f\uff0c\u8fd8\u5305\u542b\u6bcf\u6761\u6d88\u606f\uff08\u5176\u89d2\u8272\u3001\u5728\u5217\u8868\u4e2d\u7684\u4f4d\u7f6e\u7b49\uff09\u3002\u6700\u5e38\u7528\u7684\u662f <code>ChatPromptTemplates</code> \uff0c\u4e00\u4e2a\u5143\u7d20\u4e3a <code>ChatMessageTemplate</code> \u7684\u5217\u8868\u3002\u6bcf\u4e2a <code>ChatMessageTemplate</code> \u90fd\u5305\u542b\u6709\u5173\u5982\u4f55\u683c\u5f0f\u5316 <code>ChatMessage</code> \u7684\u8bf4\u660e, \u5305\u62ec\u89d2\u8272\u548c\u5185\u5bb9\u3002</p> <pre><code>import os\n\nfrom langchain.prompts.chat import ChatPromptTemplate\n\nfrom dotenv import load_dotenv\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\ndef test_chat_prompt_template():\n    template = \"\u60a8\u662f\u4e00\u4e2a\u6709\u7528\u7684\u52a9\u624b\uff0c\u53ef\u4ee5\u5c06 {input_language} \u8f6c\u6362\u4e3a {output_language}\u3002\"\n    human_template = \"{text}\"\n\n    chat_prompt = ChatPromptTemplate.from_messages([\n        (\"system\", template),\n        (\"human\", human_template),\n    ])\n\n    msg = chat_prompt.format_messages(input_language=\"English\", output_language=\"\u4e2d\u6587\", text=\"I love programming.\")\n    print(msg)\n\n\ndef main():\n    # \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\n    load_env()\n\n    test_chat_prompt_template()\n\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p></p>"},{"location":"quick_start.html#output_parser","title":"Output parser(\u8f93\u51fa\u89e3\u6790\u5668)","text":"<p><code>OutputParser</code> \u5c06\u8bed\u8a00\u6a21\u578b\u7684\u539f\u59cb\u8f93\u51fa\u8f6c\u6362\u4e3a\u53ef\u4ee5\u5728\u4e0b\u6e38\u4f7f\u7528\u7684\u683c\u5f0f\u3002 <code>OutputParser</code> \u6709\u51e0\u79cd\u4e3b\u8981\u7c7b\u578b\uff0c\u5305\u62ec\uff1a</p> <ul> <li>\u5c06 <code>LLM</code> \u751f\u6210\u7684\u6587\u672c\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u4fe1\u606f\uff08\u4f8b\u5982 JSON\uff09</li> <li>\u5c06 <code>ChatMessage</code> \u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32</li> <li>\u5c06 <code>__call__</code> \u65b9\u6cd5\u8fd4\u56de\u7684\u9664\u6d88\u606f\u4e4b\u5916\u7684\u989d\u5916\u4fe1\u606f\uff08\u5982 OpenAI \u51fd\u6570\u8c03\u7528\uff09\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\u3002</li> </ul> <p>\u6709\u5173\u8fd9\u65b9\u9762\u7684\u5b8c\u6574\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u6709\u5173\u8f93\u51fa\u89e3\u6790\u5668\u7684\u90e8\u5206\u3002</p> <p>\u5728\u672c\u5165\u95e8\u6307\u5357\u4e2d\uff0c\u6211\u4eec\u5c06\u7f16\u5199\u81ea\u5df1\u7684\u8f93\u51fa\u89e3\u6790\u5668 - \u5c06\u9017\u53f7\u5206\u9694\u5217\u8868\u8f6c\u6362\u4e3a\u5217\u8868\u7684\u89e3\u6790\u5668\u3002</p> <pre><code>import os\nfrom typing import List\n\nfrom langchain.schema import BaseOutputParser\n\nfrom dotenv import load_dotenv\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\nclass CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n    \"\"\"\u5c06 LLM \u8c03\u7528\u7684\u8f93\u51fa\u89e3\u6790\u4e3a\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\u3002\"\"\"\n\n    def parse(self, text: str) -&gt; List[str]:\n        \"\"\"\u89e3\u6790 LLM \u8c03\u7528\u7684\u8f93\u51fa\"\"\"\n        return text.strip().split(\", \")\n\n\ndef test_my_output_parser():\n    parser = CommaSeparatedListOutputParser()\n    print(parser.parse(\"hi, bye\"))\n\n\ndef main():\n    # \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\n    load_env()\n\n    test_my_output_parser()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p></p>"},{"location":"quick_start.html#lcel","title":"\u4f7f\u7528 LCEL \u7ec4\u5408","text":"<p>\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u5c06\u6240\u6709\u8fd9\u4e9b\u7ec4\u5408\u6210\u4e00\u6761\u94fe\u3002\u8be5\u94fe\u5c06\u83b7\u53d6\u8f93\u5165\u53d8\u91cf\uff0c\u5c06\u8fd9\u4e9b\u53d8\u91cf\u4f20\u9012\u7ed9\u63d0\u793a\u8bcd\u6a21\u677f\u4ee5\u521b\u5efa\u63d0\u793a\u8bcd\uff0c\u5c06\u63d0\u793a\u8bcd\u4f20\u9012\u7ed9\u8bed\u8a00\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\uff08\u53ef\u9009\uff09\u8f93\u51fa\u89e3\u6790\u5668\u4f20\u9012\u8f93\u51fa\u3002</p> <pre><code>#!/usr/bin/env python  \n# -*- coding:utf-8 -*-  \n\"\"\" \n@author: danerlt \n@file: quick_start.py\n@time: 2023-12-27\n@contact: danerlt001@gmail.com\n@desc: \u5feb\u901f\u5f00\u59cb\n\n\u53c2\u8003\u94fe\u63a5\uff1a https://python.langchain.com/docs/get_started/quickstart\n\"\"\"\nimport os\nfrom typing import List\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.schema import BaseOutputParser\n\nfrom dotenv import load_dotenv\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\nclass CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n    \"\"\"\u5c06 LLM \u8c03\u7528\u7684\u8f93\u51fa\u89e3\u6790\u4e3a\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\u3002\"\"\"\n\n    def parse(self, text: str) -&gt; List[str]:\n        \"\"\"\u89e3\u6790 LLM \u8c03\u7528\u7684\u8f93\u51fa\"\"\"\n        return text.strip().split(\", \")\n\n\ndef test_composing_with_lcel():\n    template = \"\"\"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u52a9\u624b\uff0c\u53ef\u4ee5\u751f\u6210\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\u3002\n    \u7528\u6237\u5c06\u4f20\u5165\u4e00\u4e2a\u7c7b\u522b\uff0c\u4f60\u5e94\u8be5\u5728\u5217\u8868\u4e2d\u751f\u6210 5 \u4e2a\u8be5\u7c7b\u522b\u7684\u5bf9\u8c61\u3002\n    \u53ea\u8fd4\u56de\u4e00\u4e2a\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\uff0c\u4ec5\u6b64\u800c\u5df2\u3002\"\"\"\n    human_template = \"{text}\"\n\n    chat_prompt = ChatPromptTemplate.from_messages([\n        (\"system\", template),\n        (\"human\", human_template),\n    ])\n    chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n    res = chain.invoke({\"text\": \"colors\"})\n    print(f\"res type: {type(res)}, res: {res}\")\n\n\ndef main():\n    # \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\n    load_env()\n\n    test_composing_with_lcel()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>\u7ed3\u679c\u5982\u4e0b\uff1a </p>"},{"location":"quick_start.html#langsmith_1","title":"\u4f7f\u7528LangSmith","text":"<p>\u7531 LangSmith \u8fd8\u672a\u5f00\u53d1\u6d4b\u8bd5\uff0c\u9700\u8981\u9080\u8bf7\u7801\u624d\u80fd\u4f7f\u7528\uff0c\u6682\u65f6\u4f7f\u7528\u4e0d\u4e86\u3002</p>"},{"location":"quick_start.html#langserve_1","title":"\u4f7f\u7528 LangServe \u63d0\u4f9b\u670d\u52a1","text":""},{"location":"quick_start.html#web_server","title":"web server","text":"<p>\u8981\u4e3a\u6211\u4eec\u7684\u5e94\u7528\u7a0b\u5e8f\u521b\u5efa\u4e00\u4e2aWeb\u670d\u52a1\uff0c\u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u4e09\u9879\u5185\u5bb9\u7684 <code>serve.py</code> \u6587\u4ef6\uff1a</p> <ol> <li>\u5b9a\u4e49 chain</li> <li>\u6dfb\u52a0 fastapi app</li> <li>\u901a\u8fc7 <code>langserve.add_routes</code> \u7ed9 app \u6dfb\u52a0\u8def\u7531</li> </ol> <pre><code>import os\nfrom typing import List\n\nimport uvicorn\nfrom dotenv import load_dotenv\nfrom fastapi import FastAPI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.schema import BaseOutputParser\nfrom langserve import add_routes\n\n\ndef load_env():\n    load_dotenv(verbose=True)\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        print(\"Please set OPENAI_API_KEY in your environment.\")\n        raise ValueError(\"Please set OPENAI_API_KEY in your environment.\")\n\n\nclass CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n    \"\"\"\u5c06 LLM \u8c03\u7528\u7684\u8f93\u51fa\u89e3\u6790\u4e3a\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\u3002\"\"\"\n\n    def parse(self, text: str) -&gt; List[str]:\n        \"\"\"\u89e3\u6790 LLM \u8c03\u7528\u7684\u8f93\u51fa\"\"\"\n        return text.strip().split(\", \")\n\n\ndef define_chain():\n    template = \"\"\"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u52a9\u624b\uff0c\u53ef\u4ee5\u751f\u6210\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\u3002\n    \u7528\u6237\u5c06\u4f20\u5165\u4e00\u4e2a\u7c7b\u522b\uff0c\u4f60\u5e94\u8be5\u5728\u5217\u8868\u4e2d\u751f\u6210 5 \u4e2a\u8be5\u7c7b\u522b\u7684\u5bf9\u8c61\u3002\n    \u53ea\u8fd4\u56de\u4e00\u4e2a\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\uff0c\u4ec5\u6b64\u800c\u5df2\u3002\"\"\"\n    human_template = \"{text}\"\n\n    chat_prompt = ChatPromptTemplate.from_messages([\n        (\"system\", template),\n        (\"human\", human_template),\n    ])\n    chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n    return chain\n\n\ndef define_app():\n    app = FastAPI(\n        title=\"LangChain Server\",\n        version=\"1.0\",\n        description=\"A simple API server using LangChain's Runnable interfaces\",\n    )\n    return app\n\n\ndef main():\n    # \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\n    load_env()\n    # \u5b9a\u4e49\u4e00\u4e2achain\n    chain = define_chain()\n    # \u5b9a\u4e49\u4e00\u4e2afastapi\u7684app\n    app = define_app()\n    # \u6dfb\u52a0\u8def\u7531\n    add_routes(app, chain, path=\"/category_chain\")\n    # \u542f\u52a8\u670d\u52a1\n    uvicorn.run(app, host=\"localhost\", port=8000)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>\u6267\u884c Python \u6587\u4ef6\u542f\u52a8\u670d\u52a1\u3002</p>"},{"location":"quick_start.html#playground","title":"Playground","text":"<p>\u6bcf\u4e2a LangServe \u670d\u52a1\u90fd\u5e26\u6709\u4e00\u4e2a\u7b80\u5355\u7684\u5185\u7f6e UI\uff0c\u7528\u4e8e\u914d\u7f6e\u548c\u8c03\u7528\u5177\u6709\u6d41\u8f93\u51fa\u548c\u4e2d\u95f4\u6b65\u9aa4\u53ef\u89c1\u6027\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u524d\u5f80 http://localhost:8000/category_chain/playground/ \u5c1d\u8bd5\u4e00\u4e0b\uff01</p> <p></p>"},{"location":"quick_start.html#client","title":"Client","text":"<p>\u73b0\u5728\u8ba9\u6211\u4eec\u8bbe\u7f6e\u4e00\u4e2a\u5ba2\u6237\u7aef\uff0c\u4ee5\u4fbf\u4ee5\u7f16\u7a0b\u65b9\u5f0f\u4e0e\u6211\u4eec\u7684\u670d\u52a1\u8fdb\u884c\u4ea4\u4e92\u3002\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 <code>langserve.RemoteRunnable</code> \u8f7b\u677e\u505a\u5230\u8fd9\u4e00\u70b9\u3002\u4f7f\u7528\u5b83\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0e\u670d\u52a1\u94fe\u8fdb\u884c\u4ea4\u4e92\uff0c\u5c31\u50cf\u5b83\u5728\u5ba2\u6237\u7aef\u8fd0\u884c\u4e00\u6837\u3002</p> <p><pre><code>from langserve import RemoteRunnable\n\n\ndef main():\n    remote_chain = RemoteRunnable(\"http://localhost:8000/category_chain/\")\n    res = remote_chain.invoke({\"text\": \"colors\"})\n    print(f\"type res: {type(res)}, res: {res}\")\n</code></pre> </p>"}]}